{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the Syntax guide You\n",
    "\n",
    "<!--\n",
    "\\index{syntax directed language processing}\n",
    "-->\n",
    "\n",
    "Now that we have an understanding of what parsing entails we can build our first interpreters.\n",
    "For a certain class of languages we can do our processing as soon as we recognize syntactic structures, that means\n",
    "we can do our processing right in the embedded actions of the grammar.\n",
    "This is called *syntax directed language processing* and is best illustrated with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Interpreter for Exp1 using a Recursive Descent Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{interpreter}\n",
    "\\index{interpretation}\n",
    "\\index{interpretation!algebraic terms}\n",
    "\\index{interpretation!assignment statement}\n",
    "\\index{interpretation!syntax-directed}\n",
    "\\index{syntax-directed interpretation}\n",
    "-->\n",
    "\n",
    "According to our classification of language processors in Chapter 1 an interpreter reads a program and executes the program\n",
    "directly (see Chapter 1 Figure 6).\n",
    "We accomplish this by interpreting the syntactic structures as soon as we parse them.\n",
    "This is called *syntax-directed interpretation* where we execute the semantic rules of the language as soon as we recognize\n",
    "the corresponding syntactic structures.\n",
    "\n",
    "What exactly do we mean by interpretation?\n",
    "In order to get a better idea of what interpretation is we turn to a language that you are very familiar with: algebra.\n",
    "Consider the algebraic expression,\n",
    "```\n",
    "x = 3\n",
    "```\n",
    "We interpret this expression by first interpreting the symbol `3` as the mathematical value three, we then interpret the symbol\n",
    "`x` as a variable, and because the variable appears to the left of the symbol `=`  we assign the value three to the variable `x`.\n",
    "Now consider the term,\n",
    "```\n",
    "y = 2 + x\n",
    "```\n",
    "In order to interpret this term we first figure out what value is assigned to the variable `x`, we then interpret the symbol `2` as the mathematical\n",
    "value two, and finally we compute the value of the right term by interpreting the `+` symbol as addition computing\n",
    "the value five (if we assume that `x` has the value three from the previous example).  In order to complete the interpretation of this algebraic term we again interpret\n",
    "the `=` as the assignment of the value five to the variable `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{interpretation!syntax directed}\n",
    "\\index{syntax-direct interpretation}\n",
    "-->\n",
    "\n",
    "One thing you probably noticed at this point is that the interpretation of algebraic terms is *bottom-up*, that is, it starts with the operands\n",
    "that are immediately computable, such as constant symbols or variables,  and works its way up to the top-level operator which in this case is the assignment operator.\n",
    "\n",
    ">This approach to interpretation is called syntax-directed interpretation because the interpretation is guided by the syntactic structure of the terms.\n",
    "\n",
    "Now recall the syntax of our Exp1 language:\n",
    "```\n",
    "prog : stmt_list \n",
    "\n",
    "stmt_list : stmt_list stmt\n",
    "          | stmt\n",
    "\n",
    "stmt : PRINT exp ';'\n",
    "     | STORE var exp ';'\n",
    "\n",
    "exp : '+' exp exp\n",
    "    | '-' exp exp\n",
    "    | '(' exp ')'\n",
    "    | var\n",
    "    | num\n",
    "\t\n",
    "var : NAME \n",
    "num : NUMBER\n",
    "```\n",
    "It is the language of pre-fix expressions and has two statements.  One to print values of expressions to the terminal and the other to store the value of an expression in a variable.\n",
    "In order to see what syntax-directed interpretation looks like for our Exp1 language let us start with the parse tree for the program,\n",
    "```\n",
    "store y + 2 x ;\n",
    "```\n",
    "Figure 1 shows the parse tree for this program.  It is clear from the structure of the tree that in order to compute a value to store into variable `y` we would have to interpret the tree starting at the right side leaves and then keep interpreting the operators and computing the values along the tree branches in the direction of the red arrows.  One way to visualize syntax directed interpretation is that values percolate from the tree leaves up to the root. In our case, once interpretation reaches the root of the parse tree the value computed thus far is stored in the variable `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>\n",
    "<img src=\"figures/chap03/1/figure/Slide1.jpg\" alt=\"\">\n",
    "Fig 1. Interpreting the parse tree for the program `store y + 2 x ;`.\n",
    "</center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it turns out that we can achieve the same interpretation behavior that we showed above in a parser without having to construct an explicit parse\n",
    "tree.\n",
    "Consider the non-terminal `exp` defined in\n",
    "the Exp1 grammar as,\n",
    "```\n",
    "exp : '+' exp exp\n",
    "    | '-' exp exp\n",
    "    | '(' exp ')'\n",
    "    | var\n",
    "    | num\n",
    "\n",
    "```\n",
    "Consider a hand-built recursive descent parser for this non-terminal.\n",
    "In order to enable syntax directed interpretation all we have to do is \n",
    "allow return values from the parsing functions.\n",
    "\n",
    "Consider,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp():\n",
    "    tok = token_stream.pointer()\n",
    "    \n",
    "    if tok.type == '+':\n",
    "        token_stream.next()\n",
    "        return exp() + exp()\n",
    "    \n",
    "    elif tok.type == '-':\n",
    "        token_stream.next()\n",
    "        return exp() - exp()\n",
    "    \n",
    "    elif tok.type == '(':\n",
    "        token_stream.next() # match '('\n",
    "        val = exp()\n",
    "        token_stream.next() # match ')'\n",
    "        return val\n",
    "    \n",
    "    elif tok.type == 'NAME':\n",
    "        return var()\n",
    "    \n",
    "    elif tok.type == 'NUMBER':\n",
    "        return num()\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(tok.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having declared a return value for the parsing function `exp` implies that parsing an\n",
    "expression will actually compute an integer value.\n",
    "If we look back at the interpretation of the parse tree in Figure 1 then we see that this\n",
    "is exactly what is happening: any time we see the non-terminal `exp` \n",
    "in the tree we can observe that either an integer value is being computed or propagated.\n",
    "\n",
    "<!--\n",
    "\\index{rvalue}\n",
    "-->\n",
    "\n",
    "Now, if we take a closer look at the parsing function itself we see that the exact same behavior  that we observed on the parse tree is encoded here. For the tokens `+` and `-` we see that the function `exp()` calls itself recursively and then given the returned values performs the appropriate arithmetic operation in order to compute its own return value, that is, at this point we take the two values that propagated up from the subexpressions, add or subtract them as appropriate, and return the newly computed value. Something very similar happens with the token `(`; here we simply return the value of the parenthesized expression.\n",
    "\n",
    "When we encounter variables in the expression we use the name of the variable in order to look up its associated value in our symbol table and then return that value.\n",
    "With constants we simple retun the  integer value of that constant token. \n",
    "\n",
    "The function `exp()` represents a recursive function that will recurse to the\n",
    "recursion termination cases `var()` and `num()` and then\n",
    "backs out of the recursion while returning integer values.\n",
    "In this way we see computed values percolating from the bottom up to the top where they can then be used.\n",
    "\n",
    "Here are the parsing functions for variables and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def var():\n",
    "    tok = token_stream.pointer()\n",
    "    \n",
    "    if tok.type == 'NAME':\n",
    "        token_stream.next()\n",
    "        return symbol_table.get(tok.value, 0) # return 0 if not found\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(tok.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def num():\n",
    "    tok = token_stream.pointer()\n",
    "    \n",
    "    if tok.type == 'NUMBER':\n",
    "        token_stream.next()\n",
    "        return tok.value\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(tok.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our symbol table is a dictionary in order to associate names with values,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol_table = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run those functions we need set up our lexical analysis and token stream.  For our lexical analysis we use the lexer for Exp1 from Chapter 2, `exp1_lex.lexer`.  The class `TokenStream` converts an character stream into a token stream using the given lexical analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from exp1_lex import lexer\n",
    "from grammar_stuff import TokenStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = \"+ 1 x0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: + +\n",
      "Token: NUMBER 1\n",
      "Token: NAME x0\n"
     ]
    }
   ],
   "source": [
    "while not token_stream.end_of_file():\n",
    "    tok = token_stream.pointer()\n",
    "    print(\"Token: {} {}\".format(tok.type, tok.value))\n",
    "    token_stream.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our token stream works nicely. Now, let's put this to use for parsing and evaluating Exp1 expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = \"+ 1 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Given an input stream `\"+ 1 2\"` our toplevel `exp()` call return the value `3`, as we would expect.\n",
    "\n",
    "Let's try this on something a bit more complicated,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = \"(- (+ 1 2) 1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get back to our example above, recall the grammar snippet for \n",
    "`stmt` in Exp1,\n",
    "```\n",
    "stmt : PRINT exp ';'\n",
    "     | STORE var exp ';'\n",
    "```\n",
    "The corresponding parsing function looks like this,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stmt():\n",
    "    tok = token_stream.pointer()\n",
    "    \n",
    "    if tok.type == 'PRINT':\n",
    "        token_stream.next() # match PRINT\n",
    "        print(\"> {}\".format(exp()))\n",
    "        token_stream.next() # match ;\n",
    "        return None\n",
    "    \n",
    "    elif tok.type == 'STORE':\n",
    "        token_stream.next() # match STORE\n",
    "        name = lvar()\n",
    "        val = exp()\n",
    "        symbol_table[name] = val\n",
    "        token_stream.next() # match ;\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(tok.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that in Exp1 statements themselves do not compute any values and\n",
    "therefore the corresponding parsing function does not return any values.\n",
    "Looking at the function itself we see that in the case of a `PRINT` statement we compute the value of the expression\n",
    "while parsing it and then write that value to the output.\n",
    "In terms of the `STORE` statement we parse the lvalue-variable with the function `lvar()`\n",
    "which will give us a name.\n",
    "We then parse the expression which will return an integer value for the expression and it is this value that we store in the symbol table together with the variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lvar():\n",
    "    tok = token_stream.pointer()\n",
    "    \n",
    "    if tok.type == 'NAME':\n",
    "        token_stream.next()\n",
    "        return tok.value # return var name\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(tok.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our example, we want to interpret the statement `store y + 2 x ;`.  We need to set up our input and token streams appropriately,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = \"store y + 2 x ;\"\n",
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol_table = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stmt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': 2}\n"
     ]
    }
   ],
   "source": [
    "print(symbol_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the symbol table is exactly what we had expected given that the default value for the variable `x` is zero since nothing had been assigned to it.  In order to change that we preload the symbol table with a value for `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol_table = {'x':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = \"store y + 2 x ;\"\n",
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stmt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 3, 'y': 5}\n"
     ]
    }
   ],
   "source": [
    "print(symbol_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{rvalue}\n",
    "\\index{lvalue}\n",
    "\\index{symbol table}\n",
    "-->\n",
    "\n",
    "<!-- This paragraph makes no sense here...\n",
    "Just as in the program from the previous chapter where we tried to find all the variable references were not variable definitions in an Exp0 program\n",
    "we have to be careful with the interpretation of Exp1 programs and distinguish lvalues and rvalues.\n",
    "If a variable appears as an lvalue (that is it appears as the first argument to the STORE statement) then we assign a value to it and\n",
    "if a variable appears as an rvalue (that is it appears in the expression of the STORE statement) then we just look up the corresponding \n",
    "value for the variable.\n",
    "Value updates and lookups are usually accomplished with the help of a symbol table. \n",
    "Exp1 is simple enough that a simple dictionary like table as a way to associate variable names with values suffices.\n",
    "-->\n",
    "\n",
    "The following video shows an animation of the syntax directed interpretation of our Exp1 program:\n",
    "\n",
    "<!-- videos/chap02/q7/figure.mov -->\n",
    "\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=jmE_9zOfp1g\" target=\"_blank\">\n",
    "<img style='border:1px solid #000000' src=\"movie.jpg\" width=\"120\" height=\"90\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize `x` through a `store` statement and we can print out the value of `y` with an Exp1 `print` statement.  Adding statment lists to our parser will allow us to do that.  Recall the grammar snippet that specifies statement lists,\n",
    "```\n",
    "stmt_list : stmt_list stmt\n",
    "          | stmt\n",
    "\n",
    "```\n",
    "Now we are facing a problem, this grammar snippet is not LL(1), the lookahead sets for both rules are indentical.  That means we cannot directly convert the grammar snippet into\n",
    "a recursive descent parser function.\n",
    "However, we can rewrite these rules borrowing some notation from regular expressions,\n",
    "```\n",
    "stmt_list : stmt+\n",
    "```\n",
    "meaning that a statement list consists of one or more statements.  This allows us to construct a parser function for `stmt_list`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stmt_list():\n",
    "    while True:\n",
    "        stmt()\n",
    "        if token_stream.end_of_file():\n",
    "            break\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol_table = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_stream = \\\n",
    "'''\n",
    "store x 3; \n",
    "store y + 2 x; \n",
    "print y;\n",
    "'''\n",
    "\n",
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 5\n"
     ]
    }
   ],
   "source": [
    "stmt_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a fully functioning interpreter for our Exp1 language.  The interpreter is syntax directed because the values are being computed and passed along as we are parsing the source program.  To create a more polished implementation of the interpreter we can add a toplevel driver function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exp1_rinterp(input_stream = None):\n",
    "    'A driver for our recursive descent Exp1 interpreter.'\n",
    "    \n",
    "    global token_stream\n",
    "    global symbol_table\n",
    "    \n",
    "    if not input_stream:\n",
    "        input_stream = input(\"exp1 > \")\n",
    "    \n",
    "    token_stream = TokenStream(lexer, input_stream)\n",
    "    symbol_table = dict()\n",
    "    \n",
    "    stmt_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 3\n"
     ]
    }
   ],
   "source": [
    "exp1_rinterp(\"store x 1; store y 2; print + x y;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Interpreter for Exp1 using an LR(1) Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%%%%%%% figure  %%%%%%%%%\n",
    "\\myfigureA\n",
    "{chap02:exp1interp-gram}\n",
    "{\\input{figures/chap02/22/exp1interp-gram.tex}}\n",
    "{ANTLR specification for the Exp1 interpreter.}\n",
    "\n",
    "Of course we don't want to build parsers by hand but we want to use a tool like ANTLR to generate our parsers.\n",
    "ANTLR provides the idea of an attribute that we can associate with a non-terminal and this allows\n",
    "us to encode the same parser behavior as in the hand-built parser.\n",
    "Figure~\\ref{chap02:exp1interp-gram} shows the ANTLR specification for our Exp1 interpreter.\n",
    "For space reasons we did not include the supporting Java code.  If you do look at the Java code (see the QR code below) you\n",
    "will notice that the supporting Java code is split into two different parts: one part is the \\ilisting{@header} part for the declarations of libraries to include\n",
    "in your parser and the other part is the \\ilisting{@members} parts that allows you to add data and function members to the \n",
    "parser class as we have seen before.\n",
    "\n",
    "%%%% qr code %%%%\n",
    "\\qrcode\n",
    "{Scan the QR code or use the URL in order to see full ANTLR specification for the Exp1 interpreter.}\n",
    "{qrcodes/chap02/q8/qrcode.png}\n",
    "{\\bookurl/b/2/q8/exp1Interp.g}\n",
    "\n",
    "\n",
    "If we take a closer look at the specification file and ignore the actions for a minute we\n",
    "find a number  of major differences between this specification and our original Exp1 specification in Figure~\\ref{chap02:exp1-gram}.\n",
    "The first one is that structural tokens such as {\\icd 'print'} have been put directly into the grammar specification itself.\n",
    "Due to the tight integration of the parser generator and the lexer in ANTLR, ANTLR can generate lexical rules automatically for any structural tokens appearing in the grammar specification making ANTLR specifications nice and compact and easily readable. \n",
    "The second difference is that some non-terminals now have a return value.  These are precisely the values that we are interested\n",
    "in during our interpretation of Exp1 programs and these are the same values that we saw as return values in our hand-built parser.\n",
    "And finally we have two additional lexical rules; one for comments and one for white space.  A closer look at the actions\n",
    "associated with these lexical rules shows that there is a special directive for the parser to ignore both comments and\n",
    "white space: {\\icd \\$channel=HIDDEN}.\n",
    "\n",
    "We begin by taking a look at the rules for the non-terminal {\\lstinline[basicstyle=\\normalsize]$exp$},\n",
    "\\antlrlistingnomath\n",
    "exp returns [Integer value]\n",
    "\t   \t:   '+' e1=exp e2=exp \t{ $value = $e1.value + $e2.value; }\n",
    "   \t\t|   '-' e1=exp e2=exp \t{ $value = $e1.value - $e2.value; }\n",
    "\t\t|\t'(' e=exp ')' \t\t{ $value = $e.value; }\n",
    "\t\t|\tvar \t\t\t\t{ $value = lookup($var.name); }\n",
    "\t\t|\tINTVAL\t\t\t\t{ $value = new Integer($INTVAL.text); }\n",
    "\t\t;\n",
    "\\end{lstlisting}\n",
    "The first  rule specifies the addition operation.  Notice that the original rule has two {\\lstinline[basicstyle=\\normalsize]$exp$}\n",
    "non-terminals on the right.\n",
    "This introduces an ambiguity if we are trying to access the return values of each of these non-terminals.\n",
    "In order to get rid of this ambiguity we give names to each of the non-terminals.\n",
    "In our case we call the left {\\lstinline[basicstyle=\\normalsize]$exp$} non-terminal {\\icd e1} and the right {\\lstinline[basicstyle=\\normalsize]$exp$} non-terminal {\\icd e2}.\n",
    "Recall that we declared return a value with name {\\icd value} for  every non-terminal \\ilisting{exp}.\n",
    "In the rule actions we can access these return values with a special notation. \n",
    "We can access the return value of our first expression {\\icd e1} with the notation,\n",
    "\\begin{code}\n",
    "\\$e1.value\n",
    "\\end{code}\n",
    "Similarly for expression {\\icd e2}.\n",
    "We also have a special notation to set the return value of the current {\\icd exp} non-terminal,\n",
    "\\begin{code}\n",
    "\\$value\n",
    "\\end{code}\n",
    "With this the action for the first rule simply adds the two return values for expressions {\\icd e1} and {\\icd e2} and makes\n",
    "the resulting value the return value of the current expression non-terminal,\n",
    "\\antlrlistingnomath\n",
    "{ $value = $e1.value + $e2.value; }\n",
    "\\end{lstlisting}\n",
    "You should convince yourself that the remaining rules encode exactly the same behavior as in the corresponding hand-build parsing function\n",
    "above.\n",
    "\n",
    "The next important group of rules in our grammar are the rules for the non-terminal \\ilisting{stmt},\n",
    "\\antlrlistingnomath\n",
    "stmt\t:\t'print' exp ';'\t\t\t{ print($exp.value); }\n",
    "\t\t|\t'store' var exp ';'\t\t{ update($var.name,$exp.value); }\n",
    "\t\t;\n",
    "\\end{lstlisting} \n",
    "Since statements do not return any values there is no need to declare a return value for this non-terminal.\n",
    "The first rule specifies the PRINT statement and as expected the action associated with this rule takes the value \n",
    "computed by the non-terminal {\\icd exp} and prints it.\n",
    "The second rule specifies the STORE statement and again as expected the action updates the symbol table with the name-value\n",
    "pair which consists of the variable name and the value computed by the expression.\n",
    "You should compare this rule set with hand-built parsing function for statements above.\n",
    "\n",
    "You should take notice that tokens have a built-in return value, namely the text of the string that was returned from the lexer as part of the\n",
    "token.  \n",
    "For example, in the case of the token \\ilisting{INTVAL} we can access that text with the notation,\n",
    "\\antlrlistingnomath\n",
    "$INTVAL.text\n",
    "\\end{lstlisting}\n",
    "\n",
    "All we need to do in order to complete our interpreter is to write a driver program similar to the program\n",
    "appearing in Figure~\\ref{chap02:exp0count-driver} but with the names adjusted for our new program accordingly.  \n",
    "At this point you should download the code from the book website and experiment: \\bookurl/source\n",
    "\n",
    "%%%%%%%%%%%%%%%%%%% new section %%%%%%%%%%%%%%%%%%%%%\n",
    "\\subsection{An Example: A Pretty Printer for Exp1}\n",
    "\n",
    "%%%%%%%% figure  %%%%%%%%%\n",
    "\\myfigureA\n",
    "{chap02:exp1pp-gram}\n",
    "{\\input{figures/chap02/23/exp1pp-gram.tex}}\n",
    "{ANTLR specification for the Exp1 pretty printer.}\n",
    "\n",
    "\\index{syntax directed translation}\n",
    "\\index{translation!syntax directed}\n",
    "Syntax directed language processing does not only apply to interpretation.  We can also use syntax directed techniques to build\n",
    "simple translators.\n",
    "A pretty printer for our Exp1 language is a good example to study.\n",
    "As you might know, pretty printers are programs that read the source of a program written in some programming language\n",
    "and then generate code in the same language but formatted nicely so that the program is easy to read for humans.\n",
    "This is a great example of a simple translator shown in Figure~\\ref{chap01:simple-translator} except in our case it is not necessary\n",
    "to construct an IR because we will use syntax directed translation.\n",
    "Our pretty printer accomplishes two things: One, it will put each statement on its own line.  Two, the expressions will be rewritten into\n",
    "Lisp like syntax.  \n",
    "In Lisp, each operation is embedded in a pair of parentheses.\n",
    "For example, to add two numbers in Lisp we write the following expression,\n",
    "\\begin{code}\n",
    "(+ 2 3)\n",
    "\\end{code}\n",
    "This means we also get rid of unnecessary parentheses.  For example, the expression {\\icd ((+ (2) (3)))} will be rewritten as above.\n",
    " \n",
    "Figure~\\ref{chap02:exp1pp-gram} shows the ANTLR specification of our pretty printer. \n",
    "You will notice the usual prologue in the specification.  \n",
    "Here we declare a parser member function \\ilisting{emit} that allows us to write strings to the terminal output.\n",
    "Skipping down to the lexical rules we see that nothing has changed from the specification of the syntax directed interpreter with the exception that\n",
    "now we have a token \\ilisting{VAR} instead of \\ilisting{NAME}.\n",
    "\n",
    "Now, if you look at the grammar rule section of the specification and ignore the actions for a minute\n",
    "then you will notice that we have rewritten the grammar slightly.\n",
    "It still generates the same language and in this form makes it easier to generate code.\n",
    "Also notice that none of the non-terminals have return values.\n",
    "This is because we are dealing with a simple translator, a translator that does not perform any semantic analysis but simple does a mapping\n",
    "of the syntax.\n",
    "\n",
    "The first rule of the grammar section is,\n",
    "\\antlrlistingnomath\n",
    "prog \t:\t( stmt ';' { emit(\";\\n\"); } )+ \n",
    "\t\t;\n",
    "\\end{lstlisting}\n",
    "This is the rule that states that programs consist of one or more statements.\n",
    "We have changed this rule slightly compared to the same rule in the syntax directed interpreter by inserting the semicolon token and the action that emits code.\n",
    "In this form the rule states that every time we recognize a statement followed by a semicolon in the input stream we print out a semicolon\n",
    "followed by a newline character to the output.\n",
    "This classic syntax directed language processing: the actions are dictated by the syntactic structures recognized in the input stream.\n",
    "\n",
    "The next group of rules specifies what statements look like,\n",
    "\\antlrlistingnomath\n",
    "stmt\t:\t'print' { emit(\"print \"); } exp\n",
    "\t\t|\t'store' VAR { emit(\"store \" + $VAR.text + \" \"); } exp\n",
    "\t\t;\n",
    "\\end{lstlisting}\n",
    "In the first rule we emit the keyword print as soon as we recognized the token \\ilisting{'print'} in the input stream.\n",
    "We then continue to process the input stream with the non-terminal \\ilisting{exp}.\n",
    "The second rule states that as soon as we recognized the tokens \\ilisting{'store'} and \\ilisting{VAR} we emit the keyword store and the\n",
    "variable name then continue processing with the non-terminal \\ilisting{exp}.\n",
    "\n",
    "The last group of rules in the grammar rule section specifies expressions,\n",
    "\\antlrlistingnomath\n",
    "exp\t   \t:   '+' { emit(\"(+ \"); } exp { emit(\" \"); } exp { emit(\")\"); }\n",
    "   \t\t|   '-' { emit(\"(- \"); } exp { emit(\" \"); } exp { emit(\")\"); }\n",
    "\t\t|\t'(' exp ')'\n",
    "\t\t|\tVAR \t{ emit($VAR.text); }\n",
    "\t\t|\tINTVAL \t{ emit($INTVAL.text); }\n",
    "\t\t;\n",
    "\\end{lstlisting}\n",
    "The first rule specifies the addition operation. \n",
    "Here we emit output as soon as we recognize the token \\ilisting{'+'}.\n",
    "Recall that we want to rewrite the output in Lisp format.  \n",
    "Therefore, instead of just emitting the plus sign we emit \\ilisting{\"(+ \"}, that is, an open parenthesis followed by the plus sign and a space\n",
    "character.\n",
    "We then continue processing with the first \\ilisting{exp} non-terminal.  \n",
    "Once we have recognized the syntactic structure of the corresponding expression we emit a space character and then continue processing\n",
    "with the second \\ilisting{exp} non-terminal.\n",
    "Once we have recognized the syntactic structure of this second expression we emit the closing parenthesis.\n",
    "The second rule works identically except that we are dealing with subtraction.\n",
    "Given these two rules it is easy to see that the emitted code will have a Lisp like format in that addition and subtraction operations will always\n",
    "be surrounded by parentheses.\n",
    "The third rule is interesting.\n",
    "Here we recognize the syntactic structure of parenthesized expressions but we don't emit any code for the parentheses.\n",
    "In essence we are deleting parentheses from the input program.\n",
    "These parentheses from the input program are superfluous because every non-trivial expression is already parenthesized in the output \n",
    "using the first two rules and therefore we do not emit them into the output.\n",
    "In the last two rules above we emit the strings of the recognized tokens \\ilisting{VAR} and \\ilisting{INTVAL}, respectively.\n",
    "\n",
    "In order to get a deeper insight in how syntax directed translation works is perhaps best to envision the grammar of the pretty\n",
    "printer as a recursive descent parser.\n",
    "In that case the rule set for expressions from above could be viewed as the parsing function for expressions as follows:\n",
    "\\pseudolisting\n",
    "function exp() returns void\n",
    "begin\n",
    "   switch inputToken()\n",
    "   case PLUS:\n",
    "      emit(\"(+ \") \n",
    "      exp()\n",
    "      emit(\" \")\n",
    "      exp()\n",
    "      emit(\")\")\n",
    "      return\n",
    "   case MINUS:\n",
    "      emit(\"(- \")\n",
    "      exp()\n",
    "      emit(\" \")\n",
    "      exp()\n",
    "      emit(\")\")\n",
    "      return\n",
    "   case POPEN:\n",
    "      exp()\n",
    "      matchToken(PCLOSE)\n",
    "      return\n",
    "   case VAR:\n",
    "      Token var = inputToken()\n",
    "      emit(var.getString())\n",
    "      return\n",
    "   case INTVAL:\n",
    "      Token value = inputToken()\n",
    "      emit(value.getString())\n",
    "      return\n",
    "   default:\n",
    "      syntaxError()\n",
    "   end switch\n",
    "end\n",
    "\\end{lstlisting}\n",
    "Now it is easy to see that during syntax directed translation parsing functions and code generation functions are interleaved.\n",
    "It is also easy to see that code is typically generated as soon as the relevant piece of syntax is recognized.\n",
    "See if you can work through this example using the pretty printer grammar,\n",
    "\\begin{code}\n",
    "store x 1 ; print + (x) (2) ;\n",
    "\\end{code}\n",
    "You should obtain the following output program,\n",
    "\\begin{code}\n",
    "store x 1;\n",
    "print (+ x 2);\n",
    "\\end{code}\n",
    "In order to complete our pretty printer we have to provide a driver program similar to the one in Figure~\\ref{chap02:exp0count-driver}.\n",
    "\n",
    "% TODO: look at all the recursive descent parsing code and fix the stream index - inputToken vs nextToken etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from exp1lex import PLUS, MINUS, POPEN, PCLOSE, VAR, INTVAL\n",
    "from exp1lex import input_token, match_token\n",
    "\n",
    "def exp():\n",
    "   tk = input_token()\n",
    "   if tk.name == PLUS:\n",
    "      return exp() + exp()\n",
    "   elif tk.name == MINUS:\n",
    "      return exp() - exp()\n",
    "   elif tk.name == POPEN:\n",
    "      val = exp()\n",
    "      match_token(PCLOSE)\n",
    "      return val\n",
    "   elif tk.name == VAR:\n",
    "      return symtab(tk.value)\n",
    "   elif tk.name == INTVAL:\n",
    "      return int(tk.value)\n",
    "   else:\n",
    "      syntax_error()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "17. (project) Use the code for the Exp1 language from above and extend the language with multiplication and integer division.  Demonstrate that your interpreter works by running it on some telling examples.\n",
    "\n",
    " \\ex (project)\n",
    " Rewrite the grammar in Figure~\\ref{chap02:exp1-gram} in such a way that it supports infix expressions and then construct a\n",
    " syntax directed interpreter for it.\n",
    " \n",
    " \\ex (project)\n",
    " Rewrite the grammar in Figure~\\ref{chap02:exp1-gram} in such a way that it supports\n",
    " \\begin{enumerate}\n",
    " \\item the infix operations `*' and `/', multiplication and divide, respectively, as well as addition and subtraction.\n",
    " \\item  properly encodes associativity and presence of all the operators.\n",
    " \\end{enumerate}\n",
    " and then construct a\n",
    " syntax directed interpreter for it.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
