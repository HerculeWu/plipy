{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the Syntax guide You"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{syntax directed language processing}\n",
    "-->\n",
    "\n",
    "Now that we have an understanding of what parsing entails we can build our first interpreters and simple translators.\n",
    "For a certain class of languages we can do our processing as soon as we recognize syntactic structures, that means\n",
    "we can do our processing right in the embedded actions of a grammar.\n",
    "This is called *syntax directed language processing*.\n",
    "We start this chapter by looking at a syntax directed interpreter for Exp1 based on a hand-coded recursive descent parser.\n",
    "We then implement the same interpreter by replacing the hand-coded parser with a machine generated LR(1) parser taking advantage of the embedded actions Ply provides.\n",
    "The last example we look at is a pretty printer for Exp1.  Pretty printers are an example of simple translators.  Here we implement the pretty printer in a syntax directed manner based on the embedded rules in the parser generated by Sly generated by Sly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Interpreter for Exp1 using a Recursive Descent Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{interpreter}\n",
    "\\index{interpretation}\n",
    "\\index{interpretation!algebraic terms}\n",
    "\\index{interpretation!assignment statement}\n",
    "\\index{interpretation!syntax-directed}\n",
    "\\index{syntax-directed interpretation}\n",
    "-->\n",
    "\n",
    "According to our classification of language processors in Chapter 1 an interpreter reads a program and executes the program\n",
    "directly (see Chapter 1 Figure 6).\n",
    "We accomplish this by interpreting the syntactic structures as soon as we parse them.\n",
    "This is called *syntax-directed interpretation* where we execute the semantic rules of the language as soon as we recognize\n",
    "the corresponding syntactic structures.\n",
    "\n",
    "What exactly do we mean by interpretation?\n",
    "In order to get a better idea of what interpretation is we turn to a language that you are very familiar with: algebra.\n",
    "Consider the algebraic expression,\n",
    "```\n",
    "x = 3\n",
    "```\n",
    "We interpret this expression by first interpreting the symbol `3` as the mathematical value three, we then interpret the symbol\n",
    "`x` as a variable, and because the variable appears to the left of the symbol `=`  we assign the value three to the variable `x`.\n",
    "Now consider the term,\n",
    "```\n",
    "y = 2 + x\n",
    "```\n",
    "In order to interpret this term we first figure out what value is assigned to the variable `x`, we then interpret the symbol `2` as the mathematical\n",
    "value two, and finally we compute the value of the right term by interpreting the `+` symbol as addition computing\n",
    "the value five (if we assume that `x` has the value three from the previous example).  In order to complete the interpretation of this algebraic term we again interpret\n",
    "the `=` as the assignment of the value five to the variable `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{interpretation!syntax directed}\n",
    "\\index{syntax-direct interpretation}\n",
    "-->\n",
    "\n",
    "One thing you probably noticed at this point is that the interpretation of algebraic terms is *bottom-up*, that is, it starts with the operands\n",
    "that are immediately computable, such as constant symbols or variables,  and works its way up to the top-level operator which in this case is the assignment operator.\n",
    "\n",
    ">This approach to interpretation is called syntax-directed interpretation because the interpretation is guided by the syntactic structure of the terms.\n",
    "\n",
    "We often think of this interpretation of semantics because the interpretation provides a behavioral view of the term/program.\n",
    "\n",
    "Now recall the syntax of our Exp1 language:\n",
    "```\n",
    "prog : stmt_list \n",
    "\n",
    "stmt_list : stmt_list stmt\n",
    "          | empty\n",
    "\n",
    "stmt : PRINT exp ';'\n",
    "     | STORE var exp ';'\n",
    "\n",
    "exp : '+' exp exp\n",
    "    | '-' exp exp\n",
    "    | '(' exp ')'\n",
    "    | var\n",
    "    | num\n",
    "\t\n",
    "var : NAME \n",
    "num : NUMBER\n",
    "```\n",
    "It is the language of pre-fix expressions and has two statements.  One to print values of expressions to the terminal and the other to store the value of an expression in a variable.\n",
    "In order to see what syntax-directed interpretation looks like for our Exp1 language let us start with the parse tree for the program,\n",
    "```\n",
    "store y + 2 x ;\n",
    "```\n",
    "Figure 1 shows the parse tree for this program.  It is clear from the structure of the tree that in order to compute a value to store into variable `y` we would have to interpret the tree starting at the right side leaves and then keep interpreting the operators and computing the values along the tree branches in the direction of the red arrows.  One way to visualize syntax directed interpretation is that values percolate from the tree leaves up to the root. In our case, once interpretation reaches the root of the parse tree the value computed thus far is stored in the variable `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>\n",
    "<img src=\"figures/chap03/1/figure/Slide1.jpg\" alt=\"\">\n",
    "Fig 1. Interpreting the parse tree for the program `store y + 2 x ;`.\n",
    "</center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it turns out that we can achieve the same interpretation behavior that we showed above in a parser without having to construct an explicit parse\n",
    "tree.\n",
    "Consider the non-terminal `exp` defined in\n",
    "the Exp1 grammar as,\n",
    "```\n",
    "exp : '+' exp exp\n",
    "    | '-' exp exp\n",
    "    | '(' exp ')'\n",
    "    | var\n",
    "    | num\n",
    "\n",
    "```\n",
    "We will look at a hand-built recursive descent parser for this non-terminal.\n",
    "In order to enable syntax directed interpretation all we have to do is \n",
    "allow return values from the parsing functions.\n",
    "\n",
    "Consider,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp():\n",
    "    tok = token_stream.pointer()\n",
    "    \n",
    "    if tok.type == '+':\n",
    "        token_stream.next() # match '+'\n",
    "        return exp() + exp()\n",
    "    \n",
    "    elif tok.type == '-':\n",
    "        token_stream.next() # match '-'\n",
    "        return exp() - exp()\n",
    "    \n",
    "    elif tok.type == '(':\n",
    "        token_stream.next() # match '('\n",
    "        val = exp()\n",
    "        token_stream.next() # match ')'\n",
    "        return val\n",
    "    \n",
    "    elif tok.type == 'NAME':\n",
    "        return var()\n",
    "    \n",
    "    elif tok.type == 'NUMBER':\n",
    "        return num()\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(tok.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting observation is that the function calls representing the non-terminals of the grammar implicitly build a parse tree.  Consider parsing the sub-expression `+ 1 2` that appears in the parse tree in Figure 1 with our\n",
    "`exp()` funtion.  It should be clear that pattern of function calls during parsing match exactly the non-terminals in\n",
    "the parse tree (try tracing the function calls for that expression!).  Furthermore, the return values of the parsing \n",
    "function percolate the values from the leaves of the parse tree up in such a way that the function call to `exp()` to parse `+ 1 2` will return the value three.\n",
    "\n",
    "Here is a closer look at the parsing function `exp()` itself.  For the tokens `+` and `-` we see that the function calls itself recursively twice and then given the respective returned values performs the appropriate arithmetic operation in order to compute its own return value, that is, at this point we take the two values that propagated up from the subexpressions, add or subtract them as appropriate, and return the newly computed value. \n",
    "\n",
    "Something very similar happens with the token `(`.  Here we simply return the value of the parenthesized expression.\n",
    "\n",
    "When we encounter variables and numbers in the expression call the appropriate parsing functions.\n",
    "It turns out that both of those function represent the termination cases for the recursion and provide values for the leaves.  The function `num()` simply returns the value of the number encountered and the function `var()` returns the value associated with the encountered variable name.\n",
    "\n",
    "In short, the function `exp()` represents a recursive function that will recurse while parsing expression until it\n",
    "finds either a variable to parse with `var()` or a number to parse with `num()`.\n",
    "At that point recursion stops and starts to unwind percolating values up the implicit parse tree.\n",
    "In this way we see computed values percolating from the leaves up to the root of the tree where they can then be used.\n",
    "\n",
    "Here are the parsing functions for variables and numbers.  Notice that the `var()` function looks up the value associated with the variable name in a symbol table.  Here we made the design choice that if the variable has not been previously initialized we simply return the value zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def var():\n",
    "    tok = token_stream.pointer()\n",
    "    \n",
    "    if tok.type == 'NAME':\n",
    "        token_stream.next()\n",
    "        return symbol_table.get(tok.value, 0) # return 0 if not found\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(tok.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def num():\n",
    "    tok = token_stream.pointer()\n",
    "    \n",
    "    if tok.type == 'NUMBER':\n",
    "        token_stream.next()\n",
    "        return tok.value\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(tok.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our symbol table is a dictionary in order to associate names with values,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol_table = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run those functions we need set up our lexical analysis and token stream.  For our lexical analysis we use the lexer for Exp1 from Chapter 2, `exp1_lex.lexer`.  The class `TokenStream` converts a character stream into a token stream using the given lexical analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from exp1_lex import lexer\n",
    "from grammar_stuff import TokenStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = \"+ 1 x0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a little program that illustrates how our token stream works.  It reads the tokens from the token stream one by one and prints out the token name together with its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: name=+ value=+\n",
      "Token: name=NUMBER value=1\n",
      "Token: name=NAME value=x0\n"
     ]
    }
   ],
   "source": [
    "while not token_stream.end_of_file():\n",
    "    tok = token_stream.pointer()\n",
    "    print(\"Token: name={} value={}\".format(tok.type, tok.value))\n",
    "    token_stream.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our token stream works nicely. Now, let's put this to use for parsing and evaluating Exp1 expressions using our recursive descent parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = \"+ 1 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Given an input stream `\"+ 1 2\"` our toplevel `exp()` call return the value `3`, as we would expect from our previous discussion.\n",
    "\n",
    "Let's try this on something a bit more complicated,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = \"(- (+ 1 2) 1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get back to our example above that stores a value in a variable, recall the grammar snippet for \n",
    "the non-terminal `stmt` in Exp1,\n",
    "```\n",
    "stmt : PRINT exp ';'\n",
    "     | STORE var exp ';'\n",
    "```\n",
    "The corresponding parsing function looks like this,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stmt():\n",
    "    tok = token_stream.pointer()\n",
    "    \n",
    "    if tok.type == 'PRINT':\n",
    "        token_stream.next() # match PRINT\n",
    "        print(\"> {}\".format(exp()))\n",
    "        token_stream.next() # match ;\n",
    "        return None\n",
    "    \n",
    "    elif tok.type == 'STORE':\n",
    "        token_stream.next() # match STORE\n",
    "        name = lvar() # not var()!\n",
    "        val = exp()\n",
    "        symbol_table[name] = val\n",
    "        token_stream.next() # match ;\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(tok.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that in Exp1 statements themselves do not compute any values and\n",
    "therefore the corresponding parsing function does not return any values.\n",
    "Looking at the function itself we see that in the case of a `PRINT` statement we compute the value of the expression\n",
    "while parsing it and then write that value to the output.\n",
    "In terms of the `STORE` statement we have to be careful with lvalues and rvalues of variables.  Computing rvalues of\n",
    "variables is straight forward as we saw in the `var()` function.  It is simply a matter of looking up the corresponding value in a symbol table.  In a store the name of the variable acts like an lvalue in the sense that it serves as a handle for a location to update.  In our case, the name of the variable is the key into a dictionary\n",
    "and as you see in the `stmt()` function we use that key to update the variable in the symbol table.\n",
    "So the `lvar()` function simply returns the name of the variable whereas the `var()` function returns the value associated with a variable - lvalue *vs.* rvalue.\n",
    "\n",
    "Here is the definition of the `lvar()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lvar():\n",
    "    tok = token_stream.pointer()\n",
    "    \n",
    "    if tok.type == 'NAME':\n",
    "        token_stream.next()\n",
    "        return tok.value # return var name\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(tok.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our example, we want to interpret the statement `store y + 2 x ;`.  We need to set up our input and token streams appropriately,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = \"store y + 2 x ;\"\n",
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol_table = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stmt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': 2}\n"
     ]
    }
   ],
   "source": [
    "print(symbol_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the symbol table is exactly what we had expected given that the default value for the variable `x` is zero since nothing had been assigned to it.  In order to change that we preload the symbol table with a value for `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol_table = {'x':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = \"store y + 2 x ;\"\n",
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stmt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 3, 'y': 5}\n"
     ]
    }
   ],
   "source": [
    "print(symbol_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{rvalue}\n",
    "\\index{lvalue}\n",
    "\\index{symbol table}\n",
    "-->\n",
    "\n",
    "<!-- This paragraph makes no sense here...\n",
    "Just as in the program from the previous chapter where we tried to find all the variable references were not variable definitions in an Exp0 program\n",
    "we have to be careful with the interpretation of Exp1 programs and distinguish lvalues and rvalues.\n",
    "If a variable appears as an lvalue (that is it appears as the first argument to the STORE statement) then we assign a value to it and\n",
    "if a variable appears as an rvalue (that is it appears in the expression of the STORE statement) then we just look up the corresponding \n",
    "value for the variable.\n",
    "Value updates and lookups are usually accomplished with the help of a symbol table. \n",
    "Exp1 is simple enough that a simple dictionary like table as a way to associate variable names with values suffices.\n",
    "-->\n",
    "\n",
    "The following video shows an animation of the syntax directed interpretation of our Exp1 program:\n",
    "\n",
    "<!-- videos/chap02/q7/figure.mov -->\n",
    "\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=jmE_9zOfp1g\" target=\"_blank\">\n",
    "<img style='border:1px solid #000000' src=\"movie.jpg\" width=\"120\" height=\"90\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize `x` through a `store` statement and we can print out the value of `y` with an Exp1 `print` statement.  Adding statment lists to our parser will allow us to do that.  Recall the grammar snippet that specifies statement lists,\n",
    "```\n",
    "stmt_list : stmt_list stmt\n",
    "          | empty\n",
    "\n",
    "```\n",
    "The empty rule makes it difficult to convert the grammar snippet into\n",
    "a recursive descent parser function.\n",
    "However, we can rewrite these rules borrowing some notation from regular expressions,\n",
    "```\n",
    "stmt_list : stmt*\n",
    "```\n",
    "meaning that a statement list consists of zero or more statements.  This allows us to construct a parser function for `stmt_list`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stmt_list():\n",
    "    while not token_stream.end_of_file():\n",
    "        stmt()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinitialize our symbol table and set up the streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol_table = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_stream = \\\n",
    "'''\n",
    "store x 3; \n",
    "store y + 2 x; \n",
    "print y;\n",
    "'''\n",
    "\n",
    "token_stream = TokenStream(lexer, input_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 5\n"
     ]
    }
   ],
   "source": [
    "stmt_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a fully functioning interpreter for our Exp1 language.  The interpreter is syntax directed because the values are being computed and passed along as we are parsing the source program.  To create a more polished implementation of the interpreter we can add a toplevel driver function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exp1_rinterp(input_stream = None):\n",
    "    'A driver for our recursive descent Exp1 interpreter.'\n",
    "    \n",
    "    global token_stream\n",
    "    global symbol_table\n",
    "    \n",
    "    if not input_stream:\n",
    "        input_stream = input(\"exp1 > \")\n",
    "    \n",
    "    token_stream = TokenStream(lexer, input_stream)\n",
    "    symbol_table = dict()\n",
    "    \n",
    "    stmt_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 3\n"
     ]
    }
   ],
   "source": [
    "exp1_rinterp(\"store x 1; store y 2; print + x y;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional feature to consider is to read program files rather than reading programs from strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Interpreter for Exp1 using an LR(1) Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a pretty good handle on what syntax directed interpretation entails let us implement our Exp1 interpreter using an LR(1) parser generated by Ply.\n",
    "\n",
    "Recall our Ply grammar for Exp1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load code/exp1_gram.py\n",
    "from ply import yacc\n",
    "from exp1_lex import tokens, lexer\n",
    "\n",
    "def p_grammar(_):\n",
    "    \"\"\"\n",
    "    prog : stmt_list\n",
    "    \n",
    "    stmt_list : stmt stmt_list\n",
    "              | empty\n",
    "              \n",
    "    stmt : PRINT exp ';'\n",
    "         | STORE var exp ';'\n",
    "         \n",
    "    exp : '+' exp exp\n",
    "        | '-' exp exp\n",
    "        | '(' exp ')'\n",
    "        | var\n",
    "        | num\n",
    "        \n",
    "    var : NAME\n",
    "        \n",
    "    num : NUMBER\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def p_empty(p):\n",
    "    'empty :'\n",
    "    pass\n",
    "\n",
    "def p_error(t):\n",
    "    print(\"Syntax error at '%s'\" % t.value)\n",
    "\n",
    "parser = yacc.yacc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step twoards building an interpreter with this grammar is to break all the grammar rules out into individual functions so that we can embed actions with the grammar rules.  \n",
    "The actions are just python code that are able to access individual parts of the rules.\n",
    "In particular we take advantage of the variable `p` which the parser maintains.\n",
    "\n",
    "The variable `p` that gets passed into each of the parsing functions is actually an array indexed by the tokens and non-terminals\n",
    "appearing in a grammar rule.  Consider the rule,\n",
    "```\n",
    "exp : '+' exp exp\n",
    " 0     1   2   3\n",
    "```\n",
    "We have written the index underneath each rule component.\n",
    "Here, the value of the `+` token can be accessed with `p[1]`.\n",
    "The value the first expression after the plus sign computes can be accessed with `p[2]` and the value of the second expression can be accessed with `p[3]`.\n",
    "Now, recall that LR parsers run grammar rules backwards.\n",
    "So in this case the right side of the rule, `'+' exp exp`, will be replaced by `exp` on the stack.\n",
    "Just before this happens we can assign a value to the resulting `exp` as follows,\n",
    "```\n",
    "p[0] = p[2] + p[3]\n",
    "```\n",
    "That is, just before the right side of the rule is replaced with the left side we pull the values of the expressions\n",
    "on the right side, use them to compute the value of the resulting expression, and then assign that value to the \n",
    "resulting expression with the index of zero.  In essence we emulated exactly the same computations we performed in our recursive descent parser in our generated LR(1) parser.\n",
    "\n",
    "Here is our Ply grammar with the embedded actions that make extensive use of the `p` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load code/exp1_interp_gram.py\n",
    "from ply import yacc\n",
    "from exp1_lex import tokens, lexer\n",
    "\n",
    "symbol_table = dict()\n",
    "\n",
    "def p_prog(_):\n",
    "    \"prog : stmt_list\"\n",
    "    pass\n",
    "\n",
    "def p_stmt_list(_):\n",
    "    \"\"\"\n",
    "    stmt_list : stmt stmt_list\n",
    "              | empty\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def p_print_stmt(p):\n",
    "    \"stmt : PRINT exp ';'\"\n",
    "    print(\"> {}\".format(p[2]))\n",
    "    \n",
    "def p_store_stmt(p):\n",
    "    \"stmt : STORE NAME exp ';'\"\n",
    "    symbol_table[p[2]] = p[3]\n",
    "\n",
    "def p_arith_exp(p):\n",
    "    \"\"\"\n",
    "    exp : '+' exp exp\n",
    "        | '-' exp exp\n",
    "        | '(' exp ')'\n",
    "    \"\"\"\n",
    "    if p[1] == '+':\n",
    "        p[0] = p[2] + p[3]\n",
    "    elif p[1] == '-':\n",
    "        p[0] = p[2] - p[3]\n",
    "    else: # case p[1] == '('\n",
    "        p[0] = p[2]\n",
    "\n",
    "def p_var_exp(p):\n",
    "    \"exp : var\"\n",
    "    p[0] = p[1]\n",
    "    \n",
    "def p_num_exp(p):\n",
    "    \"exp : num\"\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_var(p):\n",
    "    \"var : NAME\"\n",
    "    p[0] = symbol_table.get(p[1], 0)\n",
    "\n",
    "def p_num(p):\n",
    "    \"num : NUMBER\"\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_empty(p):\n",
    "    \"empty :\"\n",
    "    pass\n",
    "\n",
    "def p_error(t):\n",
    "    print(\"Syntax error at '%s'\" % t.value)\n",
    "\n",
    "parser = yacc.yacc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the more interesting functions to look at is `p_store_stmt` where the value of the expression `p[3]` is used\n",
    "to update the symbol table at key `p[2]`.\n",
    "Another one is `p_arith_exp` where we use the value of the first token on the left side of the rule, `p[1]`, to figure\n",
    "out what arithmetic operation we need to perform.\n",
    "\n",
    "We now import the parser from that grammar to test whether our interpreter actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Token 'NEWLINE' defined, but not used\n",
      "WARNING: There is 1 unused token\n",
      "Generating LALR tables\n"
     ]
    }
   ],
   "source": [
    "from exp1_interp_gram import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 11\n"
     ]
    }
   ],
   "source": [
    "parser.parse(input=\"store x 1; print + x 10;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, that worked!\n",
    "\n",
    "Just as in the case of our recursive descent parser, we can  provide a toplevel driver function to provide a more polished interface to our interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp1_lrinterp(input_stream = None):\n",
    "    'A driver for our recursive descent Exp1 interpreter.'\n",
    "    \n",
    "    if not input_stream:\n",
    "        input_stream = input(\"exp1 > \")\n",
    "    \n",
    "    parser.parse(input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 3\n"
     ]
    }
   ],
   "source": [
    "exp1_lrinterp(\"store x 1; store y 2; print + x y;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Pretty Printer for Exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load code/exp1_pp_gram.py\n",
    "from ply import yacc\n",
    "from exp1_lex import tokens, lexer\n",
    "\n",
    "def p_prog(p):\n",
    "    \"prog : stmt_list\"\n",
    "    print(p[1])\n",
    "\n",
    "def p_stmt_list(p):\n",
    "    \"stmt_list : stmt stmt_list\"\n",
    "    p[0] = p[1] + p[2]\n",
    "\n",
    "def p_stmt_list_empty(p):\n",
    "    \"stmt_list : empty\"\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_print_stmt(p):\n",
    "    \"stmt : PRINT exp ';'\"\n",
    "    p[0] = p[1] + p[2] +';\\n'\n",
    "\n",
    "def p_store_stmt(p):\n",
    "    \"stmt : STORE NAME exp ';'\"\n",
    "    p[0] = p[1] + ' ' + p[2] + p[3] +';\\n'\n",
    "\n",
    "def p_arith_exp(p):\n",
    "    \"\"\"\n",
    "    exp : '+' exp exp\n",
    "        | '-' exp exp\n",
    "        | '(' exp ')'\n",
    "    \"\"\"\n",
    "    if p[1] == '+':\n",
    "        p[0] = ' (+' + p[2] + p[3] + ')'\n",
    "    elif p[1] == '-':\n",
    "        p[0] = ' (-' + p[2] + p[3] + ')'\n",
    "    elif p[1] == '(':\n",
    "        p[0] = p[2]\n",
    "    else:\n",
    "        raise SyntaxError(\"parsing weirdness in expressions: {} !\".format(p[1]))\n",
    "\n",
    "def p_var_exp(p):\n",
    "    \"exp : var\"\n",
    "    p[0] = p[1]\n",
    "    \n",
    "def p_num_exp(p):\n",
    "    \"exp : num\"\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_var(p):\n",
    "    \"var : NAME\"\n",
    "    p[0] = ' ' + p[1]\n",
    "\n",
    "def p_num(p):\n",
    "    \"num : NUMBER\"\n",
    "    p[0] = ' ' + str(p[1])\n",
    "\n",
    "def p_empty(p):\n",
    "    \"empty :\"\n",
    "    p[0] = ''\n",
    "\n",
    "def p_error(t):\n",
    "    print(\"Syntax error at '%s'\" % t.value)\n",
    "\n",
    "parser = yacc.yacc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from exp1_pp_gram import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store x 1;\n",
      "store y 2;\n",
      "print (+ x y);\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser.parse(\"store x 1; store y 2; print + x y;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{syntax directed translation}\n",
    "\\index{translation!syntax directed}\n",
    "-->\n",
    "\n",
    "Syntax directed language processing does not only apply to interpretation.  We can also use syntax directed techniques to build\n",
    "simple translators.\n",
    "A pretty printer for our Exp1 language is a good example to look at.\n",
    "As you might know, pretty printers are programs that read the source of a program written in some programming language\n",
    "and then generate code in the same language but formatted nicely so that the program is easy to read for humans.\n",
    "This is a great example of a simple translator shown in Figure 8 Chapter 1 except in our case it is not necessary\n",
    "to construct an IR because we will use syntax directed translation.\n",
    "Our pretty printer accomplishes two things: \n",
    "\n",
    "1. It will put each statement on its own line.  \n",
    "\n",
    "2. The expressions will be rewritten into Lisp like syntax.  In Lisp, each operation is embedded in a pair of parentheses.  For example, to add two numbers in Lisp we write the following expression,\n",
    "```\n",
    "(+ 2 3)\n",
    "```\n",
    "This means we also get rid of unnecessary parentheses.  For example, the expression {\\icd ((+ (2) (3)))} will be rewritten as above.\n",
    " \n",
    "Figure~\\ref{chap02:exp1pp-gram} shows the ANTLR specification of our pretty printer. \n",
    "You will notice the usual prologue in the specification.  \n",
    "Here we declare a parser member function \\ilisting{emit} that allows us to write strings to the terminal output.\n",
    "Skipping down to the lexical rules we see that nothing has changed from the specification of the syntax directed interpreter with the exception that\n",
    "now we have a token \\ilisting{VAR} instead of \\ilisting{NAME}.\n",
    "\n",
    "Now, if you look at the grammar rule section of the specification and ignore the actions for a minute\n",
    "then you will notice that we have rewritten the grammar slightly.\n",
    "It still generates the same language and in this form makes it easier to generate code.\n",
    "Also notice that none of the non-terminals have return values.\n",
    "This is because we are dealing with a simple translator, a translator that does not perform any semantic analysis but simple does a mapping\n",
    "of the syntax.\n",
    "\n",
    "The first rule of the grammar section is,\n",
    "\\antlrlistingnomath\n",
    "prog \t:\t( stmt ';' { emit(\";\\n\"); } )+ \n",
    "\t\t;\n",
    "\\end{lstlisting}\n",
    "This is the rule that states that programs consist of one or more statements.\n",
    "We have changed this rule slightly compared to the same rule in the syntax directed interpreter by inserting the semicolon token and the action that emits code.\n",
    "In this form the rule states that every time we recognize a statement followed by a semicolon in the input stream we print out a semicolon\n",
    "followed by a newline character to the output.\n",
    "This classic syntax directed language processing: the actions are dictated by the syntactic structures recognized in the input stream.\n",
    "\n",
    "The next group of rules specifies what statements look like,\n",
    "\\antlrlistingnomath\n",
    "stmt\t:\t'print' { emit(\"print \"); } exp\n",
    "\t\t|\t'store' VAR { emit(\"store \" + $VAR.text + \" \"); } exp\n",
    "\t\t;\n",
    "\\end{lstlisting}\n",
    "In the first rule we emit the keyword print as soon as we recognized the token \\ilisting{'print'} in the input stream.\n",
    "We then continue to process the input stream with the non-terminal \\ilisting{exp}.\n",
    "The second rule states that as soon as we recognized the tokens \\ilisting{'store'} and \\ilisting{VAR} we emit the keyword store and the\n",
    "variable name then continue processing with the non-terminal \\ilisting{exp}.\n",
    "\n",
    "The last group of rules in the grammar rule section specifies expressions,\n",
    "\\antlrlistingnomath\n",
    "exp\t   \t:   '+' { emit(\"(+ \"); } exp { emit(\" \"); } exp { emit(\")\"); }\n",
    "   \t\t|   '-' { emit(\"(- \"); } exp { emit(\" \"); } exp { emit(\")\"); }\n",
    "\t\t|\t'(' exp ')'\n",
    "\t\t|\tVAR \t{ emit($VAR.text); }\n",
    "\t\t|\tINTVAL \t{ emit($INTVAL.text); }\n",
    "\t\t;\n",
    "\\end{lstlisting}\n",
    "The first rule specifies the addition operation. \n",
    "Here we emit output as soon as we recognize the token \\ilisting{'+'}.\n",
    "Recall that we want to rewrite the output in Lisp format.  \n",
    "Therefore, instead of just emitting the plus sign we emit \\ilisting{\"(+ \"}, that is, an open parenthesis followed by the plus sign and a space\n",
    "character.\n",
    "We then continue processing with the first \\ilisting{exp} non-terminal.  \n",
    "Once we have recognized the syntactic structure of the corresponding expression we emit a space character and then continue processing\n",
    "with the second \\ilisting{exp} non-terminal.\n",
    "Once we have recognized the syntactic structure of this second expression we emit the closing parenthesis.\n",
    "The second rule works identically except that we are dealing with subtraction.\n",
    "Given these two rules it is easy to see that the emitted code will have a Lisp like format in that addition and subtraction operations will always\n",
    "be surrounded by parentheses.\n",
    "The third rule is interesting.\n",
    "Here we recognize the syntactic structure of parenthesized expressions but we don't emit any code for the parentheses.\n",
    "In essence we are deleting parentheses from the input program.\n",
    "These parentheses from the input program are superfluous because every non-trivial expression is already parenthesized in the output \n",
    "using the first two rules and therefore we do not emit them into the output.\n",
    "In the last two rules above we emit the strings of the recognized tokens \\ilisting{VAR} and \\ilisting{INTVAL}, respectively.\n",
    "\n",
    "In order to get a deeper insight in how syntax directed translation works is perhaps best to envision the grammar of the pretty\n",
    "printer as a recursive descent parser.\n",
    "In that case the rule set for expressions from above could be viewed as the parsing function for expressions as follows:\n",
    "\\pseudolisting\n",
    "function exp() returns void\n",
    "begin\n",
    "   switch inputToken()\n",
    "   case PLUS:\n",
    "      emit(\"(+ \") \n",
    "      exp()\n",
    "      emit(\" \")\n",
    "      exp()\n",
    "      emit(\")\")\n",
    "      return\n",
    "   case MINUS:\n",
    "      emit(\"(- \")\n",
    "      exp()\n",
    "      emit(\" \")\n",
    "      exp()\n",
    "      emit(\")\")\n",
    "      return\n",
    "   case POPEN:\n",
    "      exp()\n",
    "      matchToken(PCLOSE)\n",
    "      return\n",
    "   case VAR:\n",
    "      Token var = inputToken()\n",
    "      emit(var.getString())\n",
    "      return\n",
    "   case INTVAL:\n",
    "      Token value = inputToken()\n",
    "      emit(value.getString())\n",
    "      return\n",
    "   default:\n",
    "      syntaxError()\n",
    "   end switch\n",
    "end\n",
    "\\end{lstlisting}\n",
    "Now it is easy to see that during syntax directed translation parsing functions and code generation functions are interleaved.\n",
    "It is also easy to see that code is typically generated as soon as the relevant piece of syntax is recognized.\n",
    "See if you can work through this example using the pretty printer grammar,\n",
    "\\begin{code}\n",
    "store x 1 ; print + (x) (2) ;\n",
    "\\end{code}\n",
    "You should obtain the following output program,\n",
    "\\begin{code}\n",
    "store x 1;\n",
    "print (+ x 2);\n",
    "\\end{code}\n",
    "In order to complete our pretty printer we have to provide a driver program similar to the one in Figure~\\ref{chap02:exp0count-driver}.\n",
    "\n",
    "% TODO: look at all the recursive descent parsing code and fix the stream index - inputToken vs nextToken etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographic Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "17. (project) Use the code for the Exp1 language from above and extend the language with multiplication and integer division.  Demonstrate that your interpreter works by running it on some telling examples.\n",
    "\n",
    " \\ex (project)\n",
    " Rewrite the grammar in Figure~\\ref{chap02:exp1-gram} in such a way that it supports infix expressions and then construct a\n",
    " syntax directed interpreter for it.\n",
    " \n",
    " \\ex (project)\n",
    " Rewrite the grammar in Figure~\\ref{chap02:exp1-gram} in such a way that it supports\n",
    " \\begin{enumerate}\n",
    " \\item the infix operations `*' and `/', multiplication and divide, respectively, as well as addition and subtraction.\n",
    " \\item  properly encodes associativity and presence of all the operators.\n",
    " \\end{enumerate}\n",
    " and then construct a\n",
    " syntax directed interpreter for it.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
